{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c792e7-94f5-459c-badc-a6ae37eaf14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import os\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import PIL\n",
    "import json\n",
    "\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "# from PIL import Image\n",
    "\n",
    "# import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops.boxes import box_iou\n",
    "\n",
    "from utils import Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92a86a-60ad-4b9f-86a1-02eb051d64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(dir_path, patterns=[None], exclusive_patterns=[None]):\n",
    "    \"\"\"\n",
    "    Returns a generator yielding files matching the given patterns\n",
    "\n",
    "    dir_path: Directory to search for files under. Defaults to current dir.\n",
    "    patterns: Patterns of files to search for. Defaults to [\"*\"]. Example: [\"*.json\", \"*.xml\"]\n",
    "    exclusive patterns: patterns of files not to serach for. Defaults to [None]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    path = dir_path\n",
    "    inclusive_path_patterns = patterns\n",
    "    exclusive_path_patterns = exclusive_patterns\n",
    "    all_files = pathlib.Path(dir_path)\n",
    "\n",
    "    filtered_set = set()\n",
    "\n",
    "    for pattern in inclusive_path_patterns:\n",
    "        filtered_set = filtered_set.union(set(all_files.rglob(pattern)))\n",
    "\n",
    "    for exclusive in exclusive_path_patterns:\n",
    "        if exclusive == None:\n",
    "\n",
    "            filtered_set = (file for file in filtered_set)\n",
    "\n",
    "        else:\n",
    "\n",
    "            filtered_set = filtered_set - set(all_files.rglob(exclusive))\n",
    "\n",
    "    # filtered_gen = (file for file in np.sort(np.array(list(filtered_set))))\n",
    "\n",
    "    return sorted(list(filtered_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5512e7-3879-483d-ae9d-e353855ef39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rois_from_json(json_file):\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            roi_file = json.load(f)\n",
    "\n",
    "\n",
    "        except Exception:\n",
    "            print(\"Error reading json file\")\n",
    "    rois = roi_file['rois']\n",
    "\n",
    "    rois= np.array(rois).squeeze()[:,:]\n",
    "\n",
    "\n",
    "    return rois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e59ffd-f3f0-4682-b808-6c66d8a4753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_rois_from_txt(txt_file):\n",
    "\n",
    "    bboxes = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        # numpy_test = np.array(f.readlines())\n",
    "        for line in f.readlines():\n",
    "            bboxes.append(line.strip().split(' '))\n",
    "\n",
    "    rois = np.array(bboxes, dtype = float)\n",
    "\n",
    "    return rois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53d4ee-30f0-498b-bc2b-d615f1a3fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = find_files('/home/sk/repo/datasets/Denovix/images/train/', ['*.png'])\n",
    "txts = find_files('/home/sk/repo/datasets/Denovix/labels/train/', ['*.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb9e05-c0f2-4fcd-8a23-2df9170f18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "image = Image.open(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed5dea-da97-493a-9337-0aa189f70407",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = read_rois_from_txt(txts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432222f0-c98c-4dcd-b90f-e0bfaae8218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = target[:, 1:].astype(np.float64)\n",
    "class_labels = target[:, 0].astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7c05e-25c2-4c7e-837f-4972e5016516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "album_transform = A.Compose([\n",
    "# A.RandomCrop(width=450, height=450),\n",
    "A.HorizontalFlip(p=0.5),\n",
    "A.RandomRotate90(p=0.25),\n",
    "# A.ColorJitter(p=0.25, brightness= .2, contrast=0, saturation=0, hue= .5),\n",
    "# A.Resize(300, 300)\n",
    "], bbox_params=A.BboxParams(format='coco', min_area=0, min_visibility=0.7, label_fields=['class_labels']))\n",
    "\n",
    "# transformed = album_transform(image=np.asarray(image), bboxes=rois, class_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59777e-19bc-409a-a5d2-ac311a0e60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = album_transform(image=np.asarray(image), bboxes=rois, class_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eccec7-ee7f-42d3-bb3b-931f16001276",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce17fde-faf0-40c3-8868-3080d55c2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d37f5-7c5a-44bb-8655-318f0dbf9809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15aade-a179-4f9d-8d18-57d66122d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = target[:, 1:].astype(np.float64)\n",
    "class_labels = target[:, 1].astype(np.float64)\n",
    "\n",
    "devider = np.zeros_like(rois) \n",
    "\n",
    "rois_copy = rois.copy()\n",
    "\n",
    "devider[:, [0, 2]] = image.width\n",
    "devider[:, [1, 3]] = image.height\n",
    "\n",
    "rois[:, 2] = rois_copy[:, 0] + rois_copy[:, 2] # get bbox x + width\n",
    "rois[:, 3] = rois_copy[:, 1] + rois_copy[:, 3]  # get bbox y + height\n",
    "normalized_rois = rois / devider  # normalize the bbox \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dec742-c945-420f-b2d9-f8ec317441be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(class_labels, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c0170-c47f-4ce2-a1f6-4cf9531f0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "= torch.tensor(class_labels, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae5e3a-857a-4576-9c1b-73746e472fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf91a0f-7e62-45fd-9254-9d3e996a52b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.tensor(normalized_rois, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339a392-458e-497c-b064-dc7123ae96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "        \n",
    "#         if self.img_aug:\n",
    "            \n",
    "#             rois = rois.tolist()\n",
    "#             class_labels = class_labels.tolist()\n",
    "\n",
    "\n",
    "#             transformed = self.img_aug(image=np.asarray(image), bboxes=rois, class_labels=class_labels)\n",
    "\n",
    "#             transformed_image = transformed['image']\n",
    "#             transformed_bboxes = transformed['bboxes']\n",
    "#             transformed_class_labels = transformed['class_labels']        \n",
    "\n",
    "#             ######### normalize rois##############\n",
    "\n",
    "#             transformed_bboxes_np = np.array(transformed_bboxes)\n",
    "\n",
    "#             devider = np.zeros_like(transformed_bboxes_np) \n",
    "\n",
    "#             devider[:, [0, 2]] = float(image.width)\n",
    "#             devider[:, [1, 3]] = float(image.height)\n",
    "\n",
    "#             transformed_bboxes_np[:, 2] = transformed_bboxes_np[:, 0] + transformed_bboxes_np[:, 2] # get bbox x + width\n",
    "#             transformed_bboxes_np[:, 3] = transformed_bboxes_np[:, 1] + transformed_bboxes_np[:, 3]  # get bbox y + height\n",
    "#             normalized_transformed_bboxes_np = transformed_bboxes_np / devider  # normalize the bbox \n",
    "\n",
    "            \n",
    "#             bboxes, labels = self.encoder.encode(torch.tensor(normalized_transformed_bboxes_np, dtype = torch.float), torch.tensor(transformed_class_labels).long())        \n",
    "\n",
    "#             tensor_img = transform(transformed_image).repeat(3, 1, 1) # make 3 channel images from gray image in tensor\n",
    "        \n",
    "#         else:\n",
    "img_resizer= transforms.Resize(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638212b0-476d-4105-b31e-f721f12219ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d074263-44b8-4546-8f7a-03da80905377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07833f0b-113c-43f5-b48a-44748b1cee83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9a6b6-e725-4ba3-ad55-0111952805ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD_Dataset(object):\n",
    "    def __init__(self, img_path_list, target_path_list, dboxes, img_aug=None):\n",
    "        # nothing special here, just internalizing the constructor parameters\n",
    "\n",
    "        self.img_aug = img_aug\n",
    "        self.img_path_list = img_path_list\n",
    "        self.target_path_list = target_path_list\n",
    "        self.dboxes = dboxes\n",
    "        self.encoder = Encoder(self.dboxes)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def _read_rois_from_json(self, json_file):\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            try:\n",
    "                roi_file = json.load(f)\n",
    "                \n",
    "\n",
    "            except Exception:\n",
    "                print(\"Error reading json file\")\n",
    "        rois = roi_file['rois']\n",
    "\n",
    "        rois= np.array(rois).squeeze()[:,:]\n",
    "\n",
    "\n",
    "        return rois\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        transform = ToTensor()\n",
    "        \n",
    "        image = PIL.Image.open(self.img_path_list[index])\n",
    "        \n",
    "        \n",
    "                \n",
    "        target = self._read_rois_from_json(self.target_path_list[index])\n",
    "        \n",
    "        rois = target[:, 1:].tolist()\n",
    "        class_labels = target[:, 1].tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        album_transform = A.Compose([\n",
    "    A.RandomCrop(width=450, height=450),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.25),\n",
    "    A.ColorJitter(p=0.25, brightness= .2, contrast=0, saturation=0, hue= .5),\n",
    "    A.Resize(300, 300)\n",
    "], bbox_params=A.BboxParams(format='coco', min_area=30, min_visibility=0.1, label_fields=['class_labels']))\n",
    "        \n",
    "        transformed = album_transform(image=np.asarray(image), bboxes=rois, class_labels=class_labels)\n",
    "        \n",
    "        transformed_image = transformed['image']\n",
    "        transformed_bboxes = transformed['bboxes']\n",
    "        transformed_class_labels = transformed['class_labels']        \n",
    "        \n",
    "        ######### normalize rois##############\n",
    "        \n",
    "        transformed_bboxes_np = np.array(transformed_bboxes)\n",
    "        \n",
    "        devider = np.zeros_like(transformed_bboxes_np) \n",
    "            \n",
    "        devider[:, [0, 2]] = float(image.width)\n",
    "        devider[:, [1, 3]] = float(image.height)\n",
    "\n",
    "        transformed_bboxes_np[:, 2] = transformed_bboxes_np[:, 0] + transformed_bboxes_np[:, 2] # get bbox x + width\n",
    "        transformed_bboxes_np[:, 3] = transformed_bboxes_np[:, 1] + transformed_bboxes_np[:, 3]  # get bbox y + height\n",
    "        normalized_transformed_bboxes_np = transformed_bboxes_np / devider  # normalize the bbox \n",
    "        \n",
    "        bboxes, labels = self.encoder.encode(torch.tensor(normalized_transformed_bboxes_np, dtype = torch.float), torch.tensor(transformed_class_labels).long())        \n",
    "        \n",
    "        tensor_img = transform(transformed_image).repeat(3, 1, 1)\n",
    "        \n",
    "        return (tensor_img, bboxes, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24bfbc-2e00-4fbf-902a-bdebc5d44b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path_list = find_files('/home/sk/Rewire_Image/Rewire_original_models/c-fos/', ['*.json'])\n",
    "img_path_list = find_files('/home/sk/Rewire_Image/Rewire_original_models/c-fos/', ['*.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559d966-63d0-4ec0-8404-21dfe712a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "dbbox = utils.generate_dboxes(model=\"ssd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f267d-a0c4-4f5f-9c7f-1e53d7a468e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SSD_Dataset(img_path_list=img_path_list, target_path_list=target_path_list, dboxes=dbbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d5df6-862c-4a06-a4b3-419abd430d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b2f34-83c9-4d94-aaed-0d00f23b5555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15c5a4-4216-4f23-b394-1753e63563d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bad7b-2db4-4983-ad2d-c0eb94ab06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import PIL\n",
    "import json\n",
    "\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm.autonotebook  import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39a38c-87ec-4d50-88de-1d469df4c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    items = list(zip(*batch))\n",
    "    items[0] = default_collate([i for i in items[0] if torch.is_tensor(i)])\n",
    "    # items[1] = list([i for i in items[1] if i])\n",
    "    # items[2] = list([i for i in items[2] if i])\n",
    "    items[1] = default_collate([i for i in items[1] if torch.is_tensor(i)])\n",
    "    items[2] = default_collate([i for i in items[2] if torch.is_tensor(i)])\n",
    "    return items\n",
    "\n",
    "\n",
    "class CocoDataset(CocoDetection):\n",
    "    def __init__(self, root, mode, transform=None):\n",
    "        annFile = os.path.join(root, \"annotations\", \"instances_{}.json\".format(mode))\n",
    "        root = os.path.join(root, \"{}\".format(mode))\n",
    "        super(CocoDataset, self).__init__(root, annFile)\n",
    "        self._load_categories()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _load_categories(self):\n",
    "\n",
    "        categories = self.coco.loadCats(self.coco.getCatIds())\n",
    "        categories.sort(key=lambda x: x[\"id\"])\n",
    "\n",
    "        self.label_map = {}\n",
    "        self.label_info = {}\n",
    "        counter = 1\n",
    "        self.label_info[0] = \"background\"\n",
    "        for c in categories:\n",
    "            self.label_map[c[\"id\"]] = counter\n",
    "            self.label_info[counter] = c[\"name\"]\n",
    "            counter += 1\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, target = super(CocoDataset, self).__getitem__(item)\n",
    "        width, height = image.size\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if len(target) == 0:\n",
    "            return None, None, None, None, None\n",
    "        for annotation in target:\n",
    "            bbox = annotation.get(\"bbox\")\n",
    "            # boxes.append([bbox[0] / width, bbox[1] / height, (bbox[0] + bbox[2]) / width, (bbox[1] + bbox[3]) / height])\n",
    "            boxes.append([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "            labels.append(self.label_map[annotation.get(\"category_id\")])\n",
    "        boxes = torch.tensor(np.array(boxes))\n",
    "        labels = torch.tensor(labels)\n",
    "        if self.transform is not None:\n",
    "            album_transform = A.Compose([\n",
    "                # A.RandomCrop(width=450, height=450),\n",
    "                # A.CenterCrop(p=1, height=height, width=width)],\n",
    "\n",
    "                A.LongestMaxSize(300),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomRotate90(p=0.25),\n",
    "                A.ColorJitter(p=0.25, brightness= .2, contrast=0, saturation=0, hue= .5),\n",
    "                A.PadIfNeeded(min_height=300, min_width=300, p=1)],\n",
    "                bbox_params=A.BboxParams(format='coco', min_area=1, min_visibility=0.05, label_fields=['class_labels']))\n",
    "            \n",
    "            transformed = album_transform(image=np.asarray(image), bboxes=boxes, class_labels=labels)\n",
    "                  \n",
    "            image = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            for i, box in enumerate(boxes):\n",
    "                boxes[i] = [box[0] / image.shape[0], box[1] / image.shape[1], (box[0] + box[2]) / image.shape[0], (box[1] + box[3]) / image.shape[1]]\n",
    "\n",
    "            labels = transformed['class_labels']\n",
    "        return (image, boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd6358-e7a9-4ade-96c5-c02e3715b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CocoDataset('/home/sk/repo/FishCam/splitted_anno_0/', 'train', transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232a0db-901f-41ef-9e0c-cf45cdc7e59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3304e9-78b5-40ac-a1f4-80af56c86e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22515bb3-d3d0-4d6a-97b1-6b22d5143389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e578081-a454-4298-acd7-4a7800bc3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =DataLoader(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a37a1-343d-4225-9d19-c8d3d3c87c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bar = tqdm(train_dataloader, desc=f\"batch Loss: \", leave=False)\n",
    "for i, (img, gloc, glabel) in enumerate(train_bar):\n",
    "    print(i)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f82186-d733-4491-9508-d5903cc97754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33487626-f786-491d-a1b0-6f95ec052e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0d537-29d9-4389-8e28-7357a2e38295",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CocoDataset('/home/sk/repo/FishCam/splitted_anno_0/', 'train', transform=True)\n",
    "train_dataloader= DataLoader(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae1dff-77b4-40c0-8e27-541357f3c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2c68d-18ba-46a3-8027-b5698d6179a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6428c-d588-4e8d-886a-f2171349270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96dfeb1-da45-4a47-8e9d-e596830dae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8dc3b-05cb-4d1b-b540-daef2f5c765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[0][2].get('category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b808667-a52e-4b27-b72c-703099e8f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b3738-5a62-443c-b236-46f0f1d50e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('/home/sk/SSD-pytorch/coco/annotations/instances_train2017.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "for i in data['emp_details']:\n",
    "    print(i)\n",
    "  \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c88975-ea5b-44de-8a61-ae3ce18956d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cb264-1d6f-48d3-9b14-49f351783044",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.train_set = src.dataset.SSD_Dataset(self.img_train_list, self.target_train_list, self.dboxes, img_aug = self.img_aug)\n",
    "self.train_loader = DataLoader(self.train_set, batch_size = self.batch_size, shuffle = True, num_workers = self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a534c-d714-4eb3-99e2-0c9335ca5c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bdba87-bf94-4c00-b4bc-2fdc82443f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDDataset():\n",
    "    def __init__(self, json_path, transform=None):\n",
    "        annFile = os.path.join(root, \"annotations\", \"instances_{}{}.json\".format(mode, year))\n",
    "        root = os.path.join(root, \"{}{}\".format(mode, year))\n",
    "        super(CocoDataset, self).__init__(root, annFile)\n",
    "        self._load_categories()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _load_categories(self):\n",
    "\n",
    "        categories = self.coco.loadCats(self.coco.getCatIds())\n",
    "        categories.sort(key=lambda x: x[\"id\"])\n",
    "\n",
    "        self.label_map = {}\n",
    "        self.label_info = {}\n",
    "        counter = 1\n",
    "        self.label_info[0] = \"background\"\n",
    "        for c in categories:\n",
    "            self.label_map[c[\"id\"]] = counter\n",
    "            self.label_info[counter] = c[\"name\"]\n",
    "            counter += 1\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, target = super(CocoDataset, self).__getitem__(item)\n",
    "        width, height = image.size\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if len(target) == 0:\n",
    "            return None, None, None, None, None\n",
    "        for annotation in target:\n",
    "            bbox = annotation.get(\"bbox\")\n",
    "            boxes.append([bbox[0] / width, bbox[1] / height, (bbox[0] + bbox[2]) / width, (bbox[1] + bbox[3]) / height])\n",
    "            labels.append(self.label_map[annotation.get(\"category_id\")])\n",
    "        boxes = torch.tensor(boxes)\n",
    "        labels = torch.tensor(labels)\n",
    "        if self.transform is not None:\n",
    "            image, (height, width), boxes, labels = self.transform(image, (height, width), boxes, labels)\n",
    "        # return image, target[0][\"image_id\"], (height, width), boxes, labels\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81154ca4-daf2-4df1-b462-b0e01b897af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ac1aa-1603-47c1-80de-10f7f97bfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3cad0b-ca40-40b9-8e82-f54e2b9884a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(dir_path, patterns=[None], exclusive_patterns=[None]):\n",
    "    \"\"\"\n",
    "    Returns a generator yielding files matching the given patterns\n",
    "\n",
    "    dir_path: Directory to search for files under. Defaults to current dir.\n",
    "    patterns: Patterns of files to search for. Defaults to [\"*\"]. Example: [\"*.json\", \"*.xml\"]\n",
    "    exclusive patterns: patterns of files not to serach for. Defaults to [None]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    path = dir_path\n",
    "    inclusive_path_patterns = patterns\n",
    "    exclusive_path_patterns = exclusive_patterns\n",
    "    all_files = pathlib.Path(dir_path)\n",
    "\n",
    "    filtered_set = set()\n",
    "\n",
    "    for pattern in inclusive_path_patterns:\n",
    "        filtered_set = filtered_set.union(set(all_files.rglob(pattern)))\n",
    "\n",
    "    for exclusive in exclusive_path_patterns:\n",
    "        if exclusive == None:\n",
    "\n",
    "            filtered_set = (file for file in filtered_set)\n",
    "\n",
    "        else:\n",
    "\n",
    "            filtered_set = filtered_set - set(all_files.rglob(exclusive))\n",
    "\n",
    "    # filtered_gen = (file for file in np.sort(np.array(list(filtered_set))))\n",
    "\n",
    "    return sorted(list(filtered_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3dd83-ed45-487e-a924-1564fd2f4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SSD_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98702775-94c7-4df3-b555-e67b75f08aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path_list = find_files('/home/sk/Rewire_Image/Rewire_original_models/c-fos/', ['*.json'])\n",
    "img_path_list = find_files('/home/sk/Rewire_Image/Rewire_original_models/c-fos/', ['*.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7393e-80ca-4d2c-a55f-93c9d8ca3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd_dataset= SSD_Dataset(img_path_list=img_path_list, target_path_list=target_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3404c-84c3-4f8b-8280-590f16dd556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1982502-836f-49ad-b6f1-b6ebd7c05e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DataLoader(ssd_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187be105-9845-4b03-9188-b7123ec0a448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10718267-7641-46a3-92f7-3e5becf9eaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d3b15-2ec0-4786-8633-7690a03fb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = SSD_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d01f6-be82-4cae-87d2-b12b93e53f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.imshow(aaa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725e4d3-560c-416b-844f-44bd4476389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b7b8b-cccd-4a01-beed-6e8009b86be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.PadIfNeeded(min_height= patch_size, min_width=patch_size, p=1.0),\n",
    "    A.RandomResizedCrop(height=patch_size, width=patch_size, p = 0.3),\n",
    "    A.Rotate(crop_border=False),\n",
    "    A.ColorJitter(brightness= .2, contrast=0, saturation=0, hue= .5),\n",
    "    A.RandomBrightnessContrast(p=0.3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5367cc-6b60-4d3e-94c5-3736741fd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(img_data_path, mask_data_path, img_aug, edge_weight):\n",
    "    train_X_data = find_files(img_data_path, ['*.jpg', '*.png', '*.tif', '*.tiff', '*.npy'])\n",
    "    train_y_data = find_files(mask_data_path, ['*.jpg', '*.png', '*.tif', '*.tiff', '*.npy'])\n",
    "\n",
    "    ma = [x for x in train_y_data]\n",
    "    im = [x for x in train_X_data]\n",
    "\n",
    "    dataset = Dataset(im, ma, img_aug= img_aug, edge_weight=False)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd6752-c4df-407e-ad94-0ef4d2a55cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62271b-1cd5-4361-8ed7-a4e8f823c19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_python",
   "language": "python",
   "name": "local_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
