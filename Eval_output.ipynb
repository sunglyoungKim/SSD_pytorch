{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cacc8f-136f-4112-917a-61635e81c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e094d0-d020-4e28-bad4-ecce29aa3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imread('demo/1.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27dfd5-240f-4f63-aef8-6b4a4900be7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c01b668-363e-4cbd-a72d-50eacf4493e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk/repo/SSD_pytorch/src/process.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/home/sk/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sk/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/sk/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sk/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "# from src.dataset import CocoDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from src.transform import SSDTransformer\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "from src.utils import generate_dboxes, Encoder, colors\n",
    "from src.model import SSD, ResNet\n",
    "\n",
    "import src\n",
    "import albumentations as A\n",
    "import torchmetrics\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c50f12-2f76-4e58-9c08-dd15c1d79274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _map = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "# _map.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b681ce-c994-4c7f-88b9-6f86b5d35380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35bad21f-d111-4679-8cdd-e17df241fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools, fnmatch, itertools\n",
    "import pathlib\n",
    "def find_files(dir_path: str=None, patterns: [str]=None) -> [str]:\n",
    "    \"\"\"\n",
    "    Returns a generator yielding files matching the given patterns\n",
    "    :type dir_path: str\n",
    "    :type patterns: [str]\n",
    "    :rtype : [str]\n",
    "    :param dir_path: Directory to search for files/directories under. Defaults to current dir.\n",
    "    :param patterns: Patterns of files to search for. Defaults to [\"*\"]. Example: [\"*.json\", \"*.xml\"]\n",
    "    \"\"\"\n",
    "    path = dir_path or \".\"\n",
    "    path_patterns = patterns or [\"*\"]\n",
    "    fn = []\n",
    "    for root_dir, dir_names, file_names in os.walk(path):\n",
    "\n",
    "        filter_partial = functools.partial(fnmatch.filter, file_names)\n",
    "\n",
    "        for file_name in itertools.chain(*map(filter_partial, path_patterns)):\n",
    "             fn.append(pathlib.Path(os.path.join(root_dir, file_name)))\n",
    "\n",
    "    return sorted(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3374067a-8ebb-4ba0-8027-ba4090b6cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SSD(backbone=ResNet(), num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4ac1e3-65d2-48a6-9b19-c3452f53f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/home/sk/SSD-pytorch/trained_models/Invivo_heavy_aug_train/best_SSD_Invivo_heavy_aug_train.pth')\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "dboxes = generate_dboxes(model=\"ssd\")\n",
    "encoder = Encoder(dboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9903e673-2437-4937-b0b7-235eb93ec519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/home/sk/Rewire_Image/Rewire_original_models/c-fos/test/'\n",
    "\n",
    "# img_path_list = find_files(img_path, ['*.jpg'])\n",
    "# target_path_list = find_files(img_path, ['*.json'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d35caf4-27ca-4b7c-b3fa-1328eccc3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Invivo_val_transform = A.Compose([\n",
    "    A.LongestMaxSize(300),\n",
    "    A.PadIfNeeded(300,300, border_mode=0)],\n",
    "    bbox_params=A.BboxParams(format='coco', min_area=15, min_visibility=0.1, label_fields=['class_labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b8a918-a141-4381-874e-22c804656f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank6_video2/111023_19fish_tank6_video2_00007.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank6_video2/111023_19fish_tank6_video2_00004.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank6_video2/111023_19fish_tank6_video2_00003.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank6_video2/111023_19fish_tank6_video2_00001.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank6_video1/111023_19fish_tank6_video1_00006.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank5_video2/111023_19fish_tank5_video2_00007.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank5_video1/111023_19fish_tank5_video1_00000.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank4_video2/111023_19fish_tank4_video2_00006.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank4_video2/111023_19fish_tank4_video2_00002.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank3_video1/111023_19fish_tank3_video1_00002.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank1_video2/111023_19fish_tank1_video2_00008.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank1_video2/111023_19fish_tank1_video2_00007.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank1_video2/111023_19fish_tank1_video2_00000.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank1_video1/111023_19fish_tank1_video1_00005.png\n",
      "Invivo/sampled_images/sampled_low_images/111023_19fish_tank1_video1/111023_19fish_tank1_video1_00003.png\n"
     ]
    }
   ],
   "source": [
    "train_path = '/home/sk/repo/FishCam/splitted_anno_0/'\n",
    "\n",
    "train_set = src.coco_dataset.CocoTestDataset(train_path, 'val', dboxes, img_aug = Invivo_val_transform)\n",
    "train_loader = DataLoader(train_set, batch_size = 16, shuffle = False, num_workers = 20)\n",
    "\n",
    "output_path = 'Invivo_heavy_aug_test'\n",
    "\n",
    "if os.path.isdir(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "os.makedirs(output_path)\n",
    "\n",
    "nms_threshold = 0.2\n",
    "cls_threshold = 0.0005\n",
    "\n",
    "total_19_predictions = []\n",
    "total_20_predictions = []\n",
    "\n",
    "for i, (img, img_id, img_size, gtbox, gtlabel) in enumerate(train_set):\n",
    "    if img is None:\n",
    "        continue\n",
    "    if torch.cuda.is_available():\n",
    "        img = img.cuda()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ploc, plabel = model(img.unsqueeze(dim=0))\n",
    "            \n",
    "        result = encoder.decode_batch(ploc, plabel, nms_threshold, max_output= 30)[0]\n",
    "        loc, label, prob = [r.cpu().numpy() for r in result]\n",
    "        best = np.argwhere(prob > cls_threshold).squeeze(axis=1)\n",
    "        loc = loc[best]\n",
    "        label = label[best]\n",
    "        prob = prob[best]\n",
    "        file_name = train_loader.dataset.coco.dataset['images'][i]['file_name']\n",
    "        if len(loc) > 0:\n",
    "            np_img = img.permute(1, 2, 0).cpu().numpy()\n",
    "            opencv_img = cv2.cvtColor(cv2.normalize(np_img, None, 255, 0, cv2.NORM_MINMAX, cv2.CV_8U), cv2.COLOR_RGB2BGR)\n",
    "            # path = test_set.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "            height, width= np_img.shape[:2]\n",
    "            loc[:, 0::2] *= width\n",
    "            loc[:, 1::2] *= height\n",
    "            gtbox = np.array(gtbox, dtype = np.uint32)\n",
    "            gtbox[:, 2:] = gtbox[:, :2] + gtbox[:, 2:]\n",
    "            \n",
    "            loc = loc.astype(np.int32)\n",
    "            if file_name.find('19') != -1:\n",
    "                print(file_name)\n",
    "                total_19_predictions.append(loc)\n",
    "            else:\n",
    "                total_20_predictions.append(loc)\n",
    "            # gtbox = gtbox.int()\n",
    "            \n",
    "            for box, lb, pr in zip(loc, label, prob):\n",
    "                \n",
    "                category = 'Fish'\n",
    "                \n",
    "                color = colors[lb]\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                opencv_img= cv2.rectangle(opencv_img, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "\n",
    "\n",
    "            for box in gtbox:\n",
    "                \n",
    "                category = 'GT'\n",
    "                \n",
    "                color = colors[4]\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                opencv_img= cv2.rectangle(opencv_img, (xmin, ymin), (xmax, ymax), color, 1)\n",
    "                # text_size = cv2.getTextSize(category + \" : %.2f\" % pr, cv2.FONT_HERSHEY_PLAIN, 0.01, 1)[0]\n",
    "                # cv2.rectangle(opencv_img, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, 1)\n",
    "                # cv2.putText(\n",
    "                #     opencv_img, category + \" : %.2f\" % pr,\n",
    "                #     (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                #     (255, 255, 255), 1)\n",
    "            cv2.imwrite(\"{}/{}_prediction.jpg\".format(output_path, i), opencv_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0004b-51d0-4243-ac20-4a08865fc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.coco.dataset['images'][0]['file_name'].find('19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142d5a6-a4d9-4132-a4c3-4f023d130cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_19 = np.array([len(f) for f in total_19_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c1f55-ee87-4dfa-9644-e10df75fdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_20 = np.array([len(f) for f in total_20_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355f33d-e020-47d3-978e-e7fd0d9a81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5d664-b5d5-4375-b215-59ebf6258f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame([pred_19, pred_20]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3bd1a8-d529-4876-a753-a59627fd4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.columns = ['19', '20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd7fed-afc8-457b-a16a-0acfbcabb01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0d622-ca71-49e1-a15a-b6c8e55cda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_20_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dd9c1-effa-44e5-9ef7-a656e1cbf512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4052784-99a2-4367-b729-5d7695cdc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.coco.dataset['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8114b-afe1-42cd-90cb-a88b5abdbe37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002637fc-b8b9-4d1f-9cb6-b2f64c797831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0fd195-15fd-42de-b34b-dd1fee29d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "# _map.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b980f-59f1-4d50-ac04-04fb76b7c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de0231-3f68-4532-9a70-be7fc08f14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_map.update(preds=loc, target=gtbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507e896-2107-42d7-9012-04398a2da561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbaff8-05ae-4466-9192-a267461d54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.coco.dataset['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa8286-a88c-45ef-831e-422ed6e04345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f687ddd-58a1-42a3-b944-fc40fb3bfd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in train_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283a7b1-1e1d-49a8-8907-eda7cb29ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_set:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd2242-1e33-4a98-931a-a88357692fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162d701-b1b3-4954-a8cc-1f00eaea5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51e552-8c83-4978-9a3f-3c97b94413c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtbox.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a488e4-ef4c-4d40-bea2-9a485c596a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844411b2-e3aa-4e05-a669-1cd45425fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtlabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862d059-9efa-418f-b874-6c0141e6e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d3eb4-63aa-4c05-835c-f92c535b5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_result = encoder.decode_batch(gtbox, gtlabel, nms_threshold, max_output= 30)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ac8a9-70a1-4823-8541-9789c521552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for box in gtbox.numpy():\n",
    "    \n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    try:\n",
    "        opencv_img =cv2.rectangle(opencv_img, (xmin, ymin), (xmax, ymax), colors[3], 1)\n",
    "    except:\n",
    "        print('except: ', n)\n",
    "        n+=1\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45621ac4-6385-46ab-9788-ae360ccfd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.coco.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2f377-7055-4ed3-a02e-6bbfb42da281",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(opencv_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ad696-d619-4415-a373-7872d63b33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3b952-2ba0-4b33-a58e-69dfcb632dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros([2, 4], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2aad8-9f31-4af6-bb3c-de7a33bf01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "    for i in range(batch_size):\n",
    "        targets.append(\n",
    "            dict(\n",
    "                boxes=annotations[\"bbox\"][i][:, [1, 0, 3, 2]],\n",
    "                labels=annotations[\"cls\"][i],\n",
    "            )\n",
    "        )\n",
    "# self.map.update(preds=preds, target=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd5ee4-8806-4e11-bb2f-bd732015b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89b26d-f238-4dc2-b070-eae236540dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b2000-487a-4be4-be77-e0601118998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtbox[:, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df284c-8f0d-4eee-94dd-a17a8399dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtbox[:, 0::2] *=width\n",
    "gtbox[:, 1::2] *=height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a038f-b9dc-4d36-8e37-48ef8347348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da575a-ccda-4780-acd6-660fb331ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e93e8-9dc2-4c8c-a9ab-4eab5a5b19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a283c-3250-4dc3-9a6c-53a896d6464b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb62ff4-461f-4694-845e-ce72f3473239",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603641f1-5939-4db7-bfa3-ff47fce7061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "# from apex import amp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001297c4-d890-4052-a9e6-7acc173c65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/sk/repo/FishCam/splitted_anno_0/'\n",
    "\n",
    "train_set = src.coco_dataset.CocoTestDataset(train_path, 'val', dboxes, img_aug = Invivo_val_transform)\n",
    "train_loader= DataLoader(train_set, batch_size = 10, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ae7f6-827c-4a20-9109-5a888a36df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272893b-df4c-4c59-b0c9-2a24084f4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.coco.loadRes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11b9dd-4798-479a-bd7b-825a86be6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCOeval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d639dca-8824-4e82-ba5d-cafdd4885a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = train_loader.dataset.coco.getCatIds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4f9a1-1437-48b4-b0e5-6c99d7001575",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85487f2d-7975-4b2d-98e5-c1dca4977758",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80795b-9406-459c-947b-3ae5dc870268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, epoch, writer, encoder, nms_threshold):\n",
    "    model.eval()\n",
    "    detections = []\n",
    "    category_ids = test_loader.dataset.coco.getCatIds()\n",
    "    for nbatch, (img, img_id, img_size, _, _) in enumerate(test_loader):\n",
    "        print(\"Parsing batch: {}/{}\".format(nbatch, len(test_loader)), end=\"\\r\")\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        with torch.no_grad():\n",
    "            # Get predictions\n",
    "            ploc, plabel = model(img)\n",
    "            ploc, plabel = ploc.float(), plabel.float()\n",
    "\n",
    "            for idx in range(ploc.shape[0]):\n",
    "                ploc_i = ploc[idx, :, :].unsqueeze(0)\n",
    "                plabel_i = plabel[idx, :, :].unsqueeze(0)\n",
    "                try:\n",
    "                    result = encoder.decode_batch(ploc_i, plabel_i, nms_threshold, 200)[0]\n",
    "                except:\n",
    "                    print(\"No object detected in idx: {}\".format(idx))\n",
    "                    continue\n",
    "\n",
    "                height, width = img_size[idx]\n",
    "                loc, label, prob = [r.cpu().numpy() for r in result]\n",
    "                for loc_, label_, prob_ in zip(loc, label, prob):\n",
    "                    detections.append([img_id[idx], loc_[0] * width, loc_[1] * height, (loc_[2] - loc_[0]) * width,\n",
    "                                       (loc_[3] - loc_[1]) * height, prob_,\n",
    "                                       category_ids[label_ - 1]])\n",
    "\n",
    "    detections = np.array(detections, dtype=np.float32)\n",
    "\n",
    "    coco_eval = COCOeval(test_loader.dataset.coco, test_loader.dataset.coco.loadRes(detections), iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    writer.add_scalar(\"Test/mAP\", coco_eval.stats[0], epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee88e6-4389-4f86-a5d7-3b50832206ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1e71c-d5a1-4852-bbbb-a60be83dc0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354eb617-c5e5-4bfc-960f-2cfc179f1618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_python",
   "language": "python",
   "name": "local_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
